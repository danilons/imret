{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(fname):\n",
    "    labels = {'exclusive': 0, 'item': 1, 'sku': 2}\n",
    "    content = []\n",
    "    with open(fname, 'r') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=';')\n",
    "        for nn, row in enumerate(spamreader):\n",
    "            if len(row) == 3 and nn > 0:\n",
    "                content.append(({\"preoffer\": row[0], \"item\": row[1]}, labels[row[2]]))\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset = read_file(\"/Users/danilonunes/workspace/dataset/trainset.csv\")\n",
    "testset = read_file(\"/Users/danilonunes/workspace/dataset/testset.csv\")\n",
    "validset = read_file(\"/Users/danilonunes/workspace/dataset/validationset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilonunes/.virtualenvs/imret2/lib/python2.7/site-packages/keras/preprocessing/text.py:89: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts([\" \".join([text[\"preoffer\"], text[\"item\"]]) for text, _ in trainset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sequences(tokenizer, sequences, maxlen=5000):\n",
    "    preoffer = sequence.pad_sequences(tokenizer.sequences_to_matrix([line[\"preoffer\"] for line, _ in sequences]), maxlen=maxlen)\n",
    "    item = sequence.pad_sequences(tokenizer.sequences_to_matrix([line[\"item\"] for line, _ in sequences]), maxlen=maxlen)\n",
    "    labels = np.zeros((len(item), 3), dtype=np.int32)\n",
    "    for idx, (_, label) in enumerate(sequences):\n",
    "        labels[idx, label] = 1.\n",
    "    return preoffer, item, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_preoffer, X_train_item, y_train = pad_sequences(tokenizer, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11282, 5000), (11282, 5000), (11282, 3))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_preoffer.shape, X_train_item.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val_preoffer, X_val_item, y_val = pad_sequences(tokenizer, validset)\n",
    "X_test_preoffer, X_test_item, y_test = pad_sequences(tokenizer, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilonunes/.virtualenvs/imret2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, merge, Convolution2D, MaxPooling2D, Dropout, concatenate, Merge\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_length = 5000\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 256\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 256), kernel_initializer=\"normal\", activation=\"relu\", data_format=\"channels_last\", padding=\"valid\")`\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (4, 256), kernel_initializer=\"normal\", activation=\"relu\", data_format=\"channels_last\", padding=\"valid\")`\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (5, 256), kernel_initializer=\"normal\", activation=\"relu\", data_format=\"channels_last\", padding=\"valid\")`\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:13: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(padding=\"valid\", strides=(1, 1), data_format=\"channels_last\", pool_size=(4998, 1))`\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:14: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(padding=\"valid\", strides=(1, 1), data_format=\"channels_last\", pool_size=(4997, 1))`\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:15: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(padding=\"valid\", strides=(1, 1), data_format=\"channels_last\", pool_size=(4996, 1))`\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:17: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=3, activation=\"softmax\")`\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "# this returns a tensor\n",
    "preoffer = Input(shape=(sequence_length, ), dtype='int32')\n",
    "item = Input(shape=(sequence_length, ), dtype='int32')\n",
    "inputs = concatenate([preoffer, item])\n",
    "\n",
    "embedding = Embedding(output_dim=embedding_dim, input_dim=vocabulary_size, input_length=2 * sequence_length)(inputs)\n",
    "reshape = Reshape((2 * sequence_length, embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = Convolution2D(num_filters, filter_sizes[0], embedding_dim, border_mode='valid', init='normal', activation='relu', dim_ordering='tf')(reshape)\n",
    "conv_1 = Convolution2D(num_filters, filter_sizes[1], embedding_dim, border_mode='valid', init='normal', activation='relu', dim_ordering='tf')(reshape)\n",
    "conv_2 = Convolution2D(num_filters, filter_sizes[2], embedding_dim, border_mode='valid', init='normal', activation='relu', dim_ordering='tf')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), border_mode='valid', dim_ordering='tf')(conv_0)\n",
    "maxpool_1 = MaxPooling2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), border_mode='valid', dim_ordering='tf')(conv_1)\n",
    "maxpool_2 = MaxPooling2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), border_mode='valid', dim_ordering='tf')(conv_2)\n",
    "\n",
    "merged_tensor = merge([maxpool_0, maxpool_1, maxpool_2], mode='concat', concat_axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "# reshape = Reshape((3*num_filters,))(merged_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(output_dim=3, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(input=[preoffer, item], output=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "model.compile(optimizer=\"rmsprop\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batch = gen_batch(X_train_preoffer, X_train_item, y_train)\n",
    "valid_batch = gen_batch(X_val_preoffer, X_val_item, y_val, batch_size=4)\n",
    "test_batch = gen_batch(X_test_preoffer, X_test_item, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11282 samples, validate on 2423 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "model.fit([X_train_preoffer, X_train_item], \n",
    "          y_train, \n",
    "          batch_size=32, epochs=1, verbose=1, \n",
    "          callbacks=[checkpoint], \n",
    "          validation_data=([X_val_preoffer, X_val_item], y_val))  # starts training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate([X_test_preoffer, X_test_item], y_test, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
